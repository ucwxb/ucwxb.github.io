<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiaobao Wei</title>

  <meta name="author" content="Xiaobao Wei">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xiaobao Wei</name>
                  </p>
                  <p>I am a first-year dual PhD student at
                    <a href="http://www.is.cas.cn/">Institute of Software, Chinese Academy of Sciences</a> and <a
                      href="https://www.pku.edu.cn/">Peking University</a>,
                    supervised by <a href="https://people.ucas.ac.cn/~huichen">Prof. Hui Chen</a> from ISCAS, <a
                      href="https://www.shanghangzhang.com/">Prof. Shanghang Zhang</a> from PKU and <a
                      href="https://lu-m13.github.io/">Ming Lu</a> from Intel Labs China.
                    I received my B.S. in Robotics Engineering from <a href="https://www.buaa.edu.cn/">Beihang
                      University</a> in 2023 and obtained Beijing Distinguished Graduate Award.
                  </p>
                  <p>
                    I'm actively seeking internship opportunities that align with my research interests. If you know of
                    any openings or have recommendations, I'd greatly appreciate your input.
                  </p>
                  <p>
                    My areas of focus include neural field, 3D vision and human computer interaction.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:weixiaobao0210@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/ucwxb">Github</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=MGAz5PkAAAAJ&hl=zh-CN">Google Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/XiaobaoWei.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/XiaobaoWei.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="S3Gaussian" style="display: inline;">
                  </div>
                  <img src='images/S3Gaussian/S3Gaussian.png' alt="S3Gaussian" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./">
                    <papertitle>S<sup>3</sup>Gaussian: Self-Supervised Street Gaussians for Autonomous Driving</papertitle>
                  </a>
                  <br>
                  Nan Huang, 
                  <strong>Xiaobao Wei</strong>,
                  Wenzhao Zheng, 
                  Pengju An, 
                  Ming Lu, 
                  Wei Zhan, 
                  Masayoshi Tomizuka, 
                  Kurt Keutzer, 
                  Shanghang Zhang,
                  <br>
                  <a href="https://arxiv.org/pdf/2405.20323">Paper</a>
                  /
                  <a href="https://github.com/nnanhuang/S3Gaussian">Code</a>
                  <p></p>
                  <p>
                    We propose a self-supervised street Gaussian (S3Gaussian) method to decompose dynamic and static elements in driving scenes without costly annotations.
                  </p>
                </td>
              </tr>
              
              <!--
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="GraphAvatar" style="display: inline;">
                  </div>
                  <img src='images/GraphAvatar/GraphAvatar.png' alt="GraphAvatar" width="320" height="132">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./">
                    <papertitle>GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians</papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei</strong>,
                  Peng Chen, 
                  Ming Lu, 
                  Hui Chen, 
                  Feng Tian,
                  <br>

                  <p></p>
                  <p>
                    We propose a compact method named GraphAvatar that leverages Graph Neural Networks (GNN) to generate the 3D Gaussians for head avatar animation.
                  </p>
                </td>
              </tr>
            -->

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="MFS-Seg" style="display: inline;">
                  </div>
                  <img src='images/MFS-Seg/MFS-Seg.png' alt="MFS-Seg" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/BICLab/MFS-Seg">
                    <papertitle>[Neural Networks 2024] Multi-scale full spike pattern for semantic segmentation</papertitle>
                  </a>
                  <br>
                  Qiaoyi Su,
                  Weihua He,
                  <strong>Xiaobao Wei</strong>,
                  Bo Xu,
                  Guoqi Li,
                  <br>

                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="images/MFS-Seg/MFS-Seg.pdf">Paper</a>
                  /
                  <a href="https://github.com/BICLab/MFS-Seg">Code</a>
                  <p></p>
                  <p>
                    We propose the multi-scale and full spike segmentation network (MFS-Seg), which is based on the deep direct trained SNN and represents the first attempt to train a deep SNN with surrogate gradients for semantic segmentation.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="I-MedSAM_image" style="display: inline;">
                  </div>
                  <img src='images/I-MedSAM/I-MedSAM.png' alt="I-MedSAM" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/ucwxb/I-MedSAM">
                    <papertitle>I-MedSAM: Implicit Medical Image Segmentation with Segment Anything</papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei</strong>*,
                  Jiajun Cao*,
                  Yizhu Jin,
                  Ming Lu,
                  Guangyu Wang,
                  Shanghang Zhang,
                  <br>

                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://arxiv.org/abs/2311.17081">Paper</a>
                  /
                  <a href="https://github.com/ucwxb/I-MedSAM">Code</a>
                  <p></p>
                  <p>
                    We propose I-MedSAM, which leverages the benefits of both continuous representations and SAM, to
                    obtain better cross-domain ability and accurate boundary delineation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="NTO3D_image" style="display: inline;">
                  </div>
                  <img src='images/NTO3D/NTO3D.png' alt="NTO3D" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/ucwxb/NTO3D">
                    <papertitle>[CVPR 2024] NTO3D: Neural Target Object 3D Reconstruction with Segment Anything
                    </papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei</strong>,
                  Renrui Zhang,
                  Jiarui Wu,
                  Jiaming Li,
                  Yandong Guo,
                  Shanghang Zhang,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://arxiv.org/abs/2309.12790">Paper</a>
                  /
                  <a href="https://github.com/ucwxb/NTO3D">Code</a>
                  <p></p>
                  <p>
                    We propose NTO3D, a novel high-quality Neural Target Object 3D (NTO3D) reconstruction method, which
                    leverages the benefits of both neural field and SAM.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="DiffusionTalker_image" style="display: inline;">
                  </div>
                  <img src='images/DiffusionTalker/DiffusionTalker.png' alt="DiffusionTalker" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://chenvoid.github.io/DiffusionTalker/">
                    <papertitle>DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D Face Diffuser
                    </papertitle>
                  </a>
                  <br>
                  Peng Chen*,
                  <strong>Xiaobao Wei*</strong>,
                  Ming Lu,
                  Yitong Zhu,
                  Naiming Yao,
                  Xingyu Xiao,
                  Hui Chen,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://arxiv.org/abs/2311.16565">Paper</a>
                  /
                  <a href="https://chenvoid.github.io/DiffusionTalker/">Code</a>
                  <p></p>
                  <p>
                    We propose DiffusionTalker, a diffusion-based method that utilizes contrastive learning to
                    personalize 3D facial animation and knowledge distillation to accelerate 3D animation generation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="OV-3DET_image" style="display: inline;">
                  </div>
                  <img src='images/OV-3DET/OV-3DET.png' alt="OV-3DET" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/lyhdet/OV-3DET">
                    <papertitle>[CVPR 2023] Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
                    </papertitle>
                  </a>
                  <br>
                  Yuheng Lu*,
                  Chenfeng Xu*,
                  <strong>Xiaobao Wei</strong>,
                  Xiaodong Xie,
                  Masayoshi Tomizuka,
                  Kurt Keutzer,
                  Shanghang Zhang,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">Paper</a>
                  /
                  <a href="https://github.com/lyhdet/OV-3DET">Code</a>
                  <p></p>
                  <p>
                    We propose OV-3DET, which leverages advanced image/vision-language pre-trained models to achieve
                    Open-Vocabulary 3D point-cloud DETection.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="MTTrans_image" style="display: inline;">
                  </div>
                  <img src='images/MTTrans/MTTrans.png' alt="MTTrans" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/Lafite-Yu/MTTrans-OpenSource">
                    <papertitle>[ECCV 2022] MTTrans: Cross-domain object detection with mean teacher transformer
                    </papertitle>
                  </a>
                  <br>
                  Jinze Yu,
                  Jiaming Liu,
                  <strong>Xiaobao Wei</strong>,
                  Haoyi Zhou,
                  Yohei Nakata,
                  Denis Gudovskiy,
                  Tomoyuki Okuno,
                  Jianxin Li,
                  Kurt Keutzer,
                  Shanghang Zhang,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://www.academia.edu/download/96833534/2205.01643.pdf">Paper</a>
                  /
                  <a href="https://github.com/Lafite-Yu/MTTrans-OpenSource">Code</a>
                  <p></p>
                  <p>
                    We propose an end-to-end cross-domain detection Transformer based on the mean teacher framework,
                    MTTrans, which can fully exploit unlabeled target domain data in object detection training and
                    transfer knowledge between domains via pseudo labels.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="robot_grasp_image" style="display: inline;">
                  </div>
                  <img src='images/robot_grasp/robot_grasp.png' alt="robot_grasp" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./robot_grasp">
                    <papertitle>[ICGNC 2022] Center-of-Mass-Based Robust Grasp Pose Adaptation Using RGBD Camera and
                      Force/Torque Sensing</papertitle>
                  </a>
                  <br>
                  Shang Liu*,
                  <strong>Xiaobao Wei</strong>*,
                  Lulu Wang,
                  Jing Zhang,
                  Boyu Li,
                  Haosong Yue,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://link.springer.com/chapter/10.1007/978-981-19-6613-2_263">Paper</a>
                  <p></p>
                  <p>
                    Object dropping may occur when the robotic arm grasps objects with uneven mass distribution due to
                    additional moments generated by objects gravity. To solve this problem, we present a novel work that
                    does not require extra wrist and tactile sensors and large amounts of experiments for learning.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="formation_image" style="display: inline;">
                  </div>
                  <img src='images/formation/formation.png' alt="formation" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./formation">
                    <papertitle>[CCC 2021] Time-varying group formation-tracking control for heterogeneous multi-agent
                      systems with switching topologies and time-varying delays</papertitle>
                  </a>
                  <br>
                  Shiyu Zhou,
                  <strong>Xiaobao Wei</strong>,
                  Xiwang Dong,
                  Yongzhao Hua,
                  Zhang Ren,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://ieeexplore.ieee.org/abstract/document/9550486">Paper</a>
                  <p></p>
                  <p>
                    We investigate group formation-tracking problem for heterogeneous multi-agent systems (HMASs) with
                    both switching networks and communication delays in this paper.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

      <h2>Miscellaneous</h2>
        </p>
        <details>
      <summary>Friends (click to expand, random order)</summary>
          <ul>
          <li><a href="https://chenvoid.github.io/">Peng Chen</a></li>
          <li><a href="https://horaceyi.com/">Hongrui Yi</a></li>
          </ul>
        </details>
        </p>


    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Last updated: Nov. 2023
            <br>
            Web page design credit to <a href="https://jonbarron.info" style="font-size: 14px">Jon Barron</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>
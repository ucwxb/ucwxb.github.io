<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiaobao Wei</title>

  <meta name="author" content="Xiaobao Wei">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xiaobao Wei</name>
                  </p>
                  <p>I am a second-year dual PhD student at
                    <a href="http://www.is.cas.cn/">Institute of Software, Chinese Academy of Sciences</a> and <a
                      href="https://www.pku.edu.cn/">Peking University</a>,
                    supervised by <a href="https://people.ucas.ac.cn/~huichen">Prof. Hui Chen</a> from ISCAS, <a
                      href="https://www.shanghangzhang.com/">Prof. Shanghang Zhang</a> from PKU and <a
                      href="https://lu-m13.github.io/">Ming Lu</a> from Intel Labs China.
                    I received my B.S. in Robotics Engineering from <a href="https://www.buaa.edu.cn/">Beihang
                      University</a> in 2023 and obtained Beijing Distinguished Graduate Award.
                  </p>
                  <p>
                    I serve as a reviewer for conferences and journals including CVPR, ICCV, ECCV, ICLR, NeurIPS, ICML,
                    ACM MM, ICME, AISTATS, RA-L, ICSVT.
                  </p>
                  <p>
                    I specialize in neural field and 3D vision, particularly for their integration in autonomous
                    driving, embodied AI and AI for science.
                  </p>
                  <!-- highlight with bold -->
                  <p>
                    <strong>I'm actively seeking internship opportunities that align with my research interests. If you
                      know of
                      any openings or have recommendations, I'd greatly appreciate your input.</strong>
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:weixiaobao0210@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/ucwxb">Github</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=MGAz5PkAAAAJ&hl=zh-CN">Google Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/XiaobaoWei.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/XiaobaoWei.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="EmbodiedOcc++" style="display: inline;">
                  </div>
                  <img src='images/EmbodiedOcc++/pipeline.png' alt="EmbodiedOcc++" width="320" height="150">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://pkuhaowang.github.io/EmbodiedOcc2/">
                    <papertitle>EmbodiedOcc++: Boosting Embodied 3D Occupancy Prediction with Plane Regularization and Uncertainty Sampler</papertitle>
                  </a>
                  <br>
                  Hao Wang*, <strong>Xiaobao Wei*</strong>, Xiaoan Zhang, Jianing Li, Chengyu Bai, Ying Li, Ming Lu, Wenzhao Zheng, Shanghang Zhang
                  <br>
                  <a href="https://arxiv.org/pdf/2504.09540">Paper</a>
                  /
                  <a href="https://pkuhaowang.github.io/EmbodiedOcc2/">Code</a>
                  <p></p>
                  <p>
                    We propose EmbodiedOcc++, enhancing the original framework with two key innovations: a Geometry-guided Refinement Module (GRM) that constrains Gaussian updates through plane regularization, along with a Semantic-aware Uncertainty Sampler (SUS) that enables more effective updates in overlapping regions between consecutive frames.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="ManipDreamer" style="display: inline;">
                  </div>
                  <img src='images/ManipDreamer/pipeline.png' alt="ManipDreamer" width="320" height="150">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://myendless1.github.io/ManipDreamer/">
                    <papertitle>ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance</papertitle>
                  </a>
                  <br>
                  Ying Li, <strong>Xiaobao Wei</strong>, Xiaowei Chi, Yuming Li, Zhongyu Zhao, Hao Wang, Ningning Ma, Ming Lu, Shanghang Zhang
                  <br>
                  <a href="https://arxiv.org/pdf/2504.16464">Paper</a>
                  /
                  <a href="https://myendless1.github.io/ManipDreamer/">Code</a>
                  <p></p>
                  <p>
                    We propose ManipDreamer, an advanced world model based on the action tree and visual guidance.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="DiffusionTalker_image" style="display: inline;">
                  </div>
                  <img src='images/DiffusionTalker/DiffusionTalker.png' alt="DiffusionTalker" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://chenvoid.github.io/DiffusionTalker/">
                    <papertitle>[ICME 2025]DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D Face
                      Diffuser
                    </papertitle>
                  </a>
                  <br>
                  Peng Chen,
                  <strong>Xiaobao Wei</strong>,
                  Ming Lu,
                  Yitong Zhu,
                  Naiming Yao,
                  Xingyu Xiao,
                  Hui Chen,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://arxiv.org/abs/2311.16565">Paper</a>
                  /
                  <a href="https://chenvoid.github.io/DiffusionTalker/">Code</a>
                  <p></p>
                  <p>
                    We introduce DiffusionTalker, an efficient and compact 3D face diffuser that generates personalized
                    3D facial animations based on the diffusion model.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="GraphAvatar" style="display: inline;">
                  </div>
                  <img src='images/GraphAvatar/GraphAvatar.png' alt="GraphAvatar" width="320" height="132">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/ucwxb/GraphAvatar">
                    <papertitle>[AAAI 2025] GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians
                    </papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei</strong>,
                  Peng Chen,
                  Ming Lu,
                  Hui Chen,
                  Feng Tian,
                  <br>
                  <a href="https://arxiv.org/abs/2412.13983">Paper</a>
                  /
                  <a href="https://github.com/ucwxb/GraphAvatar">Code</a>
                  <p></p>
                  <p>
                    We propose a compact method named GraphAvatar that leverages Graph Neural Networks (GNN) to generate
                    the 3D Gaussians for head avatar animation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="Mixed_image" style="display: inline;">
                  </div>
                  <img src='images/MixedGaussianAvatar/Mixed.png' alt="Mixed" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://chenvoid.github.io/MGA/">
                    <papertitle>MixedGaussianAvatar: Realistically and Geometrically Accurate Head Avatar via Mixed
                      2D-3D Gaussians</papertitle>
                  </a>
                  <br>
                  Peng Chen,
                  <strong>Xiaobao Wei</strong>,
                  Qingpo Wuwu,
                  Xinyi Wang,
                  Xingyu Xiao,
                  Ming Lu
                  <br>

                  <a href="https://arxiv.org/abs/2412.04955">Paper</a>
                  /
                  <a href="https://chenvoid.github.io/MGA/">Project</a>
                  /
                  <a href="https://github.com/ChenVoid/MGA/">Code</a>
                  <p></p>
                  <p>
                    We use 2DGS to maintain the surface geometry and employ 3DGS for color correction in areas where the
                    rendering quality of 2DGS is insufficient, reconstructing a realistically and geometrically accurate
                    3D head avatar.
                  </p>
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="EMD" style="display: inline;">
                  </div>
                  <img src='images/EMD/pipeline.png' alt="EMD" width="320" height="130">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://qingpowuwu.github.io/emdgaussian.github.io/">
                    <papertitle>[ICCV 2025] EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting</papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei*</strong>,
                  Qingpo Wuwu*,
                  Zhongyu Zhao, Zhuangzhe Wu, Nan Huang, Ming Lu, Ningning MA, Shanghang Zhang
                  <br>
                  <a href="https://arxiv.org/pdf/2411.15582">Paper</a>
                  /
                  <a href="https://qingpowuwu.github.io/emdgaussian.github.io/">Code</a>
                  <p></p>
                  <p>
                    We propose Explicit Motion Decomposition (EMD), which models the motions of dynamic objects by
                    introducing learnable motion embeddings to the Gaussians, enhancing the decomposition in street
                    scenes.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="GazeGaussian" style="display: inline;">
                  </div>
                  <img src='images/GazeGaussian/pipeline.png' alt="GazeGaussian" width="320" height="150">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ucwxb.github.io/GazeGaussian/">
                    <papertitle>[ICCV 2025] GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting</papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei</strong>, Peng Chen, Guangyu Li, <br> Ming Lu, Hui Chen, Feng Tian
                  <br>
                  <a href="https://arxiv.org/pdf/2411.12981">Paper</a>
                  /
                  <a href="https://ucwxb.github.io/GazeGaussian/">Code</a>
                  <p></p>
                  <p>
                    We propose GazeGaussian, a high-fidelity gaze redirection method that uses a two-stream 3DGS model
                    to represent the face and eye regions separately.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="PLGS" style="display: inline;">
                  </div>
                  <img src='images/PLGS/pipeline.png' alt="PLGS" width="320" height="190">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2410.17505">
                    <papertitle>[IEEE TIP 2025] PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting</papertitle>
                  </a>
                  <br>
                  Yu Wang, <strong>Xiaobao Wei</strong>, Ming Lu, Guoliang Kang
                  <br>
                  <a href="https://arxiv.org/pdf/2410.17505">Paper</a>
                  <p></p>
                  <p>
                    We propose a new method called PLGS that enables 3DGS to generate consistent panoptic segmentation
                    masks from noisy 2D segmentation masks while maintaining superior efficiency compared to NeRF-based
                    methods.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="S3Gaussian" style="display: inline;">
                  </div>
                  <img src='images/S3Gaussian/S3Gaussian.png' alt="S3Gaussian" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/nnanhuang/S3Gaussian">
                    <papertitle>S<sup>3</sup>Gaussian: Self-Supervised Street Gaussians for Autonomous Driving
                    </papertitle>
                  </a>
                  <br>
                  Nan Huang*,
                  <strong>Xiaobao Wei*</strong>,
                  Wenzhao Zheng,
                  Pengju An,
                  Ming Lu,
                  Wei Zhan,
                  Masayoshi Tomizuka,
                  Kurt Keutzer,
                  Shanghang Zhang,
                  <br>
                  <a href="https://arxiv.org/pdf/2405.20323">Paper</a>
                  /
                  <a href="https://github.com/nnanhuang/S3Gaussian">Code</a>
                  <p></p>
                  <p>
                    We propose a self-supervised street Gaussian (S3Gaussian) method to decompose dynamic and static
                    elements in driving scenes without costly annotations.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="I-MedSAM_image" style="display: inline;">
                  </div>
                  <img src='images/I-MedSAM/I-MedSAM.png' alt="I-MedSAM" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/ucwxb/I-MedSAM">
                    <papertitle>[ECCV 2024]I-MedSAM: Implicit Medical Image Segmentation with Segment Anything
                    </papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei</strong>*,
                  Jiajun Cao*,
                  Yizhu Jin,
                  Ming Lu,
                  Guangyu Wang,
                  Shanghang Zhang,
                  <br>

                  <!-- <a href="./I-MedSAM">Project</a>
  / -->
                  <a href="https://arxiv.org/abs/2311.17081">Paper</a>
                  /
                  <a href="https://github.com/ucwxb/I-MedSAM">Code</a>
                  <p></p>
                  <p>
                    We propose I-MedSAM, which leverages the benefits of both continuous representations and SAM, to
                    obtain better cross-domain ability and accurate boundary delineation.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="MFS-Seg" style="display: inline;">
                  </div>
                  <img src='images/MFS-Seg/MFS-Seg.png' alt="MFS-Seg" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/BICLab/MFS-Seg">
                    <papertitle>[Neural Networks 2024] Multi-scale full spike pattern for semantic segmentation
                    </papertitle>
                  </a>
                  <br>
                  Qiaoyi Su,
                  Weihua He,
                  <strong>Xiaobao Wei</strong>,
                  Bo Xu,
                  Guoqi Li,
                  <br>

                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="images/MFS-Seg/MFS-Seg.pdf">Paper</a>
                  /
                  <a href="https://github.com/BICLab/MFS-Seg">Code</a>
                  <p></p>
                  <p>
                    We propose the multi-scale and full spike segmentation network (MFS-Seg), which is based on the deep
                    direct trained SNN and represents the first attempt to train a deep SNN with surrogate gradients for
                    semantic segmentation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="NTO3D_image" style="display: inline;">
                  </div>
                  <img src='images/NTO3D/NTO3D.png' alt="NTO3D" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/ucwxb/NTO3D">
                    <papertitle>[CVPR 2024] NTO3D: Neural Target Object 3D Reconstruction with Segment Anything
                    </papertitle>
                  </a>
                  <br>
                  <strong>Xiaobao Wei</strong>,
                  Renrui Zhang,
                  Jiarui Wu,
                  Jiaming Li,
                  Yandong Guo,
                  Shanghang Zhang,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://arxiv.org/abs/2309.12790">Paper</a>
                  /
                  <a href="https://github.com/ucwxb/NTO3D">Code</a>
                  <p></p>
                  <p>
                    We propose NTO3D, a novel high-quality Neural Target Object 3D (NTO3D) reconstruction method, which
                    leverages the benefits of both neural field and SAM.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="OV-3DET_image" style="display: inline;">
                  </div>
                  <img src='images/OV-3DET/OV-3DET.png' alt="OV-3DET" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/lyhdet/OV-3DET">
                    <papertitle>[CVPR 2023] Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
                    </papertitle>
                  </a>
                  <br>
                  Yuheng Lu*,
                  Chenfeng Xu*,
                  <strong>Xiaobao Wei</strong>,
                  Xiaodong Xie,
                  Masayoshi Tomizuka,
                  Kurt Keutzer,
                  Shanghang Zhang,
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">Paper</a>
                  /
                  <a href="https://github.com/lyhdet/OV-3DET">Code</a>
                  <p></p>
                  <p>
                    We propose OV-3DET, which leverages advanced image/vision-language pre-trained models to achieve
                    Open-Vocabulary 3D point-cloud DETection.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="MTTrans_image" style="display: inline;">
                  </div>
                  <img src='images/MTTrans/MTTrans.png' alt="MTTrans" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/Lafite-Yu/MTTrans-OpenSource">
                    <papertitle>[ECCV 2022] MTTrans: Cross-domain object detection with mean teacher transformer
                    </papertitle>
                  </a>
                  <br>
                  Jinze Yu,
                  Jiaming Liu,
                  <strong>Xiaobao Wei</strong>,
                  Haoyi Zhou,
                  Yohei Nakata,
                  Denis Gudovskiy,
                  Tomoyuki Okuno,
                  Jianxin Li,
                  Kurt Keutzer,
                  Shanghang Zhang,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://www.academia.edu/download/96833534/2205.01643.pdf">Paper</a>
                  /
                  <a href="https://github.com/Lafite-Yu/MTTrans-OpenSource">Code</a>
                  <p></p>
                  <p>
                    We propose an end-to-end cross-domain detection Transformer based on the mean teacher framework,
                    MTTrans, which can fully exploit unlabeled target domain data in object detection training and
                    transfer knowledge between domains via pseudo labels.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="robot_grasp_image" style="display: inline;">
                  </div>
                  <img src='images/robot_grasp/robot_grasp.png' alt="robot_grasp" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./robot_grasp">
                    <papertitle>[ICGNC 2022] Center-of-Mass-Based Robust Grasp Pose Adaptation Using RGBD Camera and
                      Force/Torque Sensing</papertitle>
                  </a>
                  <br>
                  Shang Liu*,
                  <strong>Xiaobao Wei</strong>*,
                  Lulu Wang,
                  Jing Zhang,
                  Boyu Li,
                  Haosong Yue,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://link.springer.com/chapter/10.1007/978-981-19-6613-2_263">Paper</a>
                  <p></p>
                  <p>
                    Object dropping may occur when the robotic arm grasps objects with uneven mass distribution due to
                    additional moments generated by objects gravity. To solve this problem, we present a novel work that
                    does not require extra wrist and tactile sensors and large amounts of experiments for learning.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="two" id="formation_image" style="display: inline;">
                  </div>
                  <img src='images/formation/formation.png' alt="formation" width="320" height="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="./formation">
                    <papertitle>[CCC 2021] Time-varying group formation-tracking control for heterogeneous multi-agent
                      systems with switching topologies and time-varying delays</papertitle>
                  </a>
                  <br>
                  Shiyu Zhou,
                  <strong>Xiaobao Wei</strong>,
                  Xiwang Dong,
                  Yongzhao Hua,
                  Zhang Ren,
                  <br>
                  <!-- <a href="./I-MedSAM">Project</a>
    / -->
                  <a href="https://ieeexplore.ieee.org/abstract/document/9550486">Paper</a>
                  <p></p>
                  <p>
                    We investigate group formation-tracking problem for heterogeneous multi-agent systems (HMASs) with
                    both switching networks and communication delays in this paper.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <h2>Internships</h2>
          <div style="display: flex; flex-wrap: wrap;">
            <div style="flex: 0 0 33.333%; padding: 5px;"><i class="fas fa-calendar-alt"></i> <b>2025.07-now</b><br><i
              class="fas fa-building"></i> MI Â∞èÁ±≥Ê±ΩËΩ¶<br><i class="fas fa-tasks"></i> Street Gaussian for Closed-loop Simulation
            </div>
            <div style="flex: 0 0 33.333%; padding: 5px;"><i class="fas fa-calendar-alt"></i> <b>2024.07-2025.06</b><br><i
              class="fas fa-building"></i> NIO ËîöÊù•Ê±ΩËΩ¶<br><i class="fas fa-tasks"></i> 3DGS for Driving Scenes
            </div>
            <div style="flex: 0 0 33.333%; padding: 5px;"><i class="fas fa-calendar-alt"></i> <b>2024.04-2025.04</b><br><i
              class="fas fa-building"></i> Intellif ‰∫ëÂ§©Âä±È£û<br><i class="fas fa-tasks"></i> AI for Medical Image
            </div>
            <div style="flex: 0 0 33.333%; padding: 5px;"><i class="fas fa-calendar-alt"></i>
              <b>2024.01-2024.06</b><br><i class="fas fa-building"></i> AMD<br><i class="fas fa-tasks"></i> End-to-end
              Driving at Scale
            </div>
            <div style="flex: 0 0 33.333%; padding: 5px;"><i class="fas fa-calendar-alt"></i>
              <b>2023.07-2023.08</b><br><i class="fas fa-building"></i> Ai2Robotics Êô∫Âπ≥ÊñπÁßëÊäÄ<br><i
                class="fas fa-tasks"></i> NeRF for Driving Scenes
            </div>
          </div>

          <h2>Miscellaneous</h2>
          </p>
          <details>
            <summary>Friends (click to expand, random order)</summary>
            <ul>
              <li><a href="https://chenvoid.github.io/">Peng Chen</a></li>
              <li><a href="https://horaceyi.com/">Hongrui Yi</a></li>
              <li><a href="https://harrison-1eo.github.io/">Haochen Li</a></li>
            </ul>
          </details>
          </p>



    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Last updated: Nov. 2023
            <br>
            Web page design credit to <a href="https://jonbarron.info" style="font-size: 14px">Jon Barron</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>

  <script>
    var _hmt = _hmt || [];
    (function () {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9a7b4078eded07c5cf9a87f4a81496da";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


</body>

</html>